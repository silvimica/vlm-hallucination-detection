{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Demonstration\n",
    "\n",
    "This notebook demonstrates the evaluation of the models we trained on the validation dataset. MOdels are loaded using weights saved after training of a specific model. \n",
    "\n",
    "### Models Demonstrated:\n",
    "1. Logistic Regression (LogReg)\n",
    "2. SVM\n",
    "3. Multi-Layer Perceptron (MLP)\n",
    "4. \n",
    "5.\n",
    "6. \n",
    "7. Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we:\n",
    "1. Load the validation dataset.\n",
    "2. Apply the `TfidfVectorizer` for feature extraction (Saved after training).\n",
    "3. Standardize the extracted features using a `StandardScaler` (Saved after training).\n",
    "\n",
    "These steps ensure that the validation dataset is processed consistently with the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer file exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431f061f6f9c42ada2a9c9524ef8a69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add src folder to the Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom functions\n",
    "from data_processing.preprocessing import get_dataset\n",
    "from data_processing.feature_extraction import apply_features\n",
    "\n",
    "# Paths to the TF-IDF vectorizer and scaler\n",
    "tfidf_path = '../src/models/weights/tfidf_vectorizer.joblib'\n",
    "scaler_path = '../src/models/weights/standard_scaler.joblib'\n",
    "\n",
    "# Load the validation dataset\n",
    "validation_dataset = get_dataset('val')\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "if os.path.exists(tfidf_path):\n",
    "    print(\"TF-IDF Vectorizer file exists.\")\n",
    "    tfidf = joblib.load(tfidf_path)\n",
    "else:\n",
    "    print(\"TF-IDF Vectorizer file does not exist.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Apply TF-IDF and other features to the validation dataset\n",
    "validation_dataset = validation_dataset.map(lambda batch: apply_features(batch, tf_idf=tfidf), batched=True)\n",
    "\n",
    "# Convert features to a matrix format and extract labels\n",
    "validation_features = np.vstack(validation_dataset['features'])\n",
    "validation_labels = validation_dataset['label']  # Replace 'label' with the actual label column name\n",
    "\n",
    "# Load and apply the Standard Scaler\n",
    "scaler = joblib.load(scaler_path)\n",
    "validation_features = scaler.transform(validation_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (LogReg) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained Logistic Regression model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg - Accuracy: 0.7708304353873974\n",
      "LogReg - Macro F1 Score: 0.689905917584563\n",
      "LogReg - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85     10257\n",
      "           1       0.64      0.45      0.53      4121\n",
      "\n",
      "    accuracy                           0.77     14378\n",
      "   macro avg       0.72      0.68      0.69     14378\n",
      "weighted avg       0.76      0.77      0.76     14378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Path to the Logistic Regression model\n",
    "model_path = '../src/models/weights/logreg/logreg_model_weights.joblib'\n",
    "\n",
    "# Load the Logistic Regression model\n",
    "logreg = joblib.load(model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = logreg.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(validation_labels, predictions)\n",
    "print(\"LogReg - Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "macro_f1 = f1_score(validation_labels, predictions, average='macro')\n",
    "print(\"LogReg - Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"LogReg - Classification Report:\\n\", classification_report(validation_labels, predictions))\n",
    "\n",
    "# Record probabilities for ensemble \n",
    "logreg_probs = logreg.predict_proba(validation_features)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained SVM model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompression complete! The file has been saved to: ../src/models/weights/svm/svm_model_weights.joblib\n"
     ]
    }
   ],
   "source": [
    "# Unzip SVM weights (Unzipped file is too large for git repository)\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Paths to the compressed and decompressed files\n",
    "compressed_file_path = '../src/models/weights/svm/svm_model_weights.joblib.gz'\n",
    "decompressed_file_path = '../src/models/weights/svm/svm_model_weights.joblib'\n",
    "\n",
    "# Open and decompress the .gz file\n",
    "with gzip.open(compressed_file_path, 'rb') as f_in:\n",
    "    with open(decompressed_file_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Decompression complete! The file has been saved to:\", decompressed_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.7143552649881764\n",
      "SVM - Macro F1 Score: 0.6304512572863838\n",
      "SVM - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.81     10257\n",
      "           1       0.50      0.41      0.45      4121\n",
      "\n",
      "    accuracy                           0.71     14378\n",
      "   macro avg       0.64      0.62      0.63     14378\n",
      "weighted avg       0.70      0.71      0.71     14378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM) Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Path to the SVM model\n",
    "svm_model_path = '../src/models/weights/svm/svm_model_weights.joblib'\n",
    "\n",
    "# Load the SVM model\n",
    "svm_model = joblib.load(svm_model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "svm_predictions = svm_model.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "svm_accuracy = accuracy_score(validation_labels, svm_predictions)\n",
    "print(\"SVM - Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "svm_macro_f1 = f1_score(validation_labels, svm_predictions, average='macro')\n",
    "print(\"SVM - Macro F1 Score:\", svm_macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"SVM - Classification Report:\\n\", classification_report(validation_labels, svm_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained MLP model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Accuracy: 0.7292391153150647\n",
      "MLP - Macro F1 Score: 0.6545075126574034\n",
      "MLP - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82     10257\n",
      "           1       0.53      0.46      0.49      4121\n",
      "\n",
      "    accuracy                           0.73     14378\n",
      "   macro avg       0.66      0.65      0.65     14378\n",
      "weighted avg       0.72      0.73      0.72     14378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Layer Perceptron (MLP) Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Path to the MLP model\n",
    "mlp_model_path = '../src/models/weights/mlp/mlp_model_weights.joblib'\n",
    "\n",
    "# Load the MLP model\n",
    "mlp_model = joblib.load(mlp_model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "mlp_predictions = mlp_model.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "mlp_accuracy = accuracy_score(validation_labels, mlp_predictions)\n",
    "print(\"MLP - Accuracy:\", mlp_accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "mlp_macro_f1 = f1_score(validation_labels, mlp_predictions, average='macro')\n",
    "print(\"MLP - Macro F1 Score:\", mlp_macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"MLP - Classification Report:\\n\", classification_report(validation_labels, mlp_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c55ba5b04794b74a5e1d52f3ecc15cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys \n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.transformer_based_models import load_and_prepare_model, train_and_evaluate\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5859922fcf054a72b913f38f65eb718a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7818\n",
      "F1-Score: 0.7185\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import ElectraTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "import torch.nn.functional as F\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='electra')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the Electra Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained Electra model\n",
    "electra_model = load_and_prepare_model('electra', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/electra_model.pth'\n",
    "electra_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "electra_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "electra_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "electra_probs = []\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = electra_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        electra_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')  # Choose 'macro', 'micro', or 'binary' as appropriate\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Record probabilities for ensemble\n",
    "electra_probs_combined = np.concatenate(electra_probs, axis=0)  # Concatenate along the batch axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae006cae2eb4e7fb50ddda81525600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7708\n",
      "F1-Score: 0.7102\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='bert')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the BERT Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = load_and_prepare_model('bert', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/bert_model.pth'\n",
    "bert_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "bert_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "bert_probs = []\n",
    "\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        bert_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')  # Choose 'macro', 'micro', or 'binary' as appropriate\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Record probabilities for ensemble\n",
    "bert_probs_combined = np.concatenate(bert_probs, axis=0)  # Concatenate along the batch axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645b4834cdae40bfb665a3c6adaf7b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7756\n",
      "F1-Score: 0.7160\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='roberta')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the RoBERTa Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained RoBERTa model\n",
    "roberta_model = load_and_prepare_model('roberta', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/roberta_model.pth'\n",
    "roberta_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "roberta_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "roberta_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "roberta_probs = []\n",
    "\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        roberta_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')  # Choose 'macro', 'micro', or 'binary' as appropriate\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "roberta_probs_combined = np.concatenate(roberta_probs, axis=0)  # Concatenate along the batch axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softvoting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Ensemble Accuracy: 0.7837\n",
      "Soft Voting Ensemble F1-Score: 0.7194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Step 1: Stack probabilities and calculate the mean (soft voting)\n",
    "all_probs = np.stack([logreg_probs, bert_probs_combined, electra_probs_combined, roberta_probs_combined], axis=0)  # Shape: (num_models, num_samples, num_classes)\n",
    "ensemble_probs = np.mean(all_probs, axis=0)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "# Step 2: Predict the final class\n",
    "ensemble_predictions = np.argmax(ensemble_probs, axis=1)  # Shape: (num_samples,)\n",
    "\n",
    "# Ground truth labels (validation_labels)\n",
    "# Assuming validation_labels is a NumPy array of true class labels\n",
    "accuracy = accuracy_score(validation_labels, ensemble_predictions)\n",
    "f1 = f1_score(validation_labels, ensemble_predictions, average='macro')  # Use 'weighted' or 'micro' as needed\n",
    "\n",
    "print(f\"Soft Voting Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Soft Voting Ensemble F1-Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-hallucinations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
