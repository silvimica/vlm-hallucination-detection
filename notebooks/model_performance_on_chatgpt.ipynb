{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Demonstration\n",
    "\n",
    "This notebook demonstrates the evaluation of the models we trained on the validation dataset. MOdels are loaded using weights saved after training of a specific model. \n",
    "\n",
    "### Models Demonstrated:\n",
    "1. Logistic Regression (LogReg)\n",
    "2. SVM\n",
    "3. Multi-Layer Perceptron (MLP)\n",
    "4. ElectraSequenceClassifier\n",
    "5. BertSequenceClassifier\n",
    "6. RobertaSequenceClassifier\n",
    "7. Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we:\n",
    "1. Load the validation dataset.\n",
    "2. Apply the `TfidfVectorizer` for feature extraction (Saved after training).\n",
    "3. Standardize the extracted features using a `StandardScaler` (Saved after training).\n",
    "\n",
    "These steps ensure that the validation dataset is processed consistently with the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sholpan.bolatzhanova/miniconda3/envs/vlm-hal-detect/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sholpan.bolatzhanova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-11-18 21:32:53.435530: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-18 21:32:53.450684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731951173.468487 3375929 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731951173.473803 3375929 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 21:32:53.492714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer file exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 918/918 [00:01<00:00, 600.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add src folder to the Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom functions\n",
    "from data_processing.preprocessing import get_dataset, get_dataset_chatgpt\n",
    "from data_processing.feature_extraction import apply_features\n",
    "\n",
    "# Paths to the TF-IDF vectorizer and scaler\n",
    "tfidf_path = '../src/models/weights/tfidf_vectorizer.joblib'\n",
    "scaler_path = '../src/models/weights/standard_scaler.joblib'\n",
    "\n",
    "# Load the validation dataset\n",
    "validation_dataset = get_dataset_chatgpt()\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "if os.path.exists(tfidf_path):\n",
    "    print(\"TF-IDF Vectorizer file exists.\")\n",
    "    tfidf = joblib.load(tfidf_path)\n",
    "else:\n",
    "    print(\"TF-IDF Vectorizer file does not exist.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Apply TF-IDF and other features to the validation dataset\n",
    "validation_dataset = validation_dataset.map(lambda batch: apply_features(batch, tf_idf=tfidf), batched=True)\n",
    "\n",
    "# Convert features to a matrix format and extract labels\n",
    "validation_features = np.vstack(validation_dataset['features'])\n",
    "validation_labels = validation_dataset['label']  # Replace 'label' with the actual label column name\n",
    "\n",
    "# Load and apply the Standard Scaler\n",
    "scaler = joblib.load(scaler_path)\n",
    "validation_features = scaler.transform(validation_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (LogReg) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained Logistic Regression model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg - Accuracy: 0.8986928104575164\n",
      "LogReg - Macro F1 Score: 0.5128028076582875\n",
      "LogReg - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       887\n",
      "           1       0.06      0.13      0.08        31\n",
      "\n",
      "    accuracy                           0.90       918\n",
      "   macro avg       0.51      0.53      0.51       918\n",
      "weighted avg       0.94      0.90      0.92       918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Path to the Logistic Regression model\n",
    "model_path = '../src/models/weights/logreg/logreg_model_weights.joblib'\n",
    "\n",
    "# Load the Logistic Regression model\n",
    "logreg = joblib.load(model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = logreg.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(validation_labels, predictions)\n",
    "print(\"LogReg - Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "macro_f1 = f1_score(validation_labels, predictions, average='macro')\n",
    "print(\"LogReg - Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"LogReg - Classification Report:\\n\", classification_report(validation_labels, predictions))\n",
    "\n",
    "# Record probabilities for ensemble \n",
    "logreg_probs = logreg.predict_proba(validation_features)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained SVM model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompression complete! The file has been saved to: ../src/models/weights/svm/svm_model_weights.joblib\n"
     ]
    }
   ],
   "source": [
    "# Unzip SVM weights (Unzipped file is too large for git repository)\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Paths to the compressed and decompressed files\n",
    "compressed_file_path = '../src/models/weights/svm/svm_model_weights.joblib.gz'\n",
    "decompressed_file_path = '../src/models/weights/svm/svm_model_weights.joblib'\n",
    "\n",
    "# Open and decompress the .gz file\n",
    "with gzip.open(compressed_file_path, 'rb') as f_in:\n",
    "    with open(decompressed_file_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Decompression complete! The file has been saved to:\", decompressed_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.8856209150326797\n",
      "SVM - Macro F1 Score: 0.5207410390656464\n",
      "SVM - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       887\n",
      "           1       0.07      0.19      0.10        31\n",
      "\n",
      "    accuracy                           0.89       918\n",
      "   macro avg       0.52      0.55      0.52       918\n",
      "weighted avg       0.94      0.89      0.91       918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM) Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Path to the SVM model\n",
    "svm_model_path = '../src/models/weights/svm/svm_model_weights.joblib'\n",
    "\n",
    "# Load the SVM model\n",
    "svm_model = joblib.load(svm_model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "svm_predictions = svm_model.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "svm_accuracy = accuracy_score(validation_labels, svm_predictions)\n",
    "print(\"SVM - Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "svm_macro_f1 = f1_score(validation_labels, svm_predictions, average='macro')\n",
    "print(\"SVM - Macro F1 Score:\", svm_macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"SVM - Classification Report:\\n\", classification_report(validation_labels, svm_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Evaluation\n",
    "\n",
    "In this section, we:\n",
    "1. Load the trained MLP model.\n",
    "2. Use the model to predict on the validation dataset.\n",
    "3. Evaluate the model's performance using:\n",
    "    - Accuracy\n",
    "    - Macro F1 Score\n",
    "    - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Accuracy: 0.8387799564270153\n",
      "MLP - Macro F1 Score: 0.47548527572308785\n",
      "MLP - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       887\n",
      "           1       0.02      0.10      0.04        31\n",
      "\n",
      "    accuracy                           0.84       918\n",
      "   macro avg       0.49      0.48      0.48       918\n",
      "weighted avg       0.93      0.84      0.88       918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Layer Perceptron (MLP) Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Path to the MLP model\n",
    "mlp_model_path = '../src/models/weights/mlp/mlp_model_weights.joblib'\n",
    "\n",
    "# Load the MLP model\n",
    "mlp_model = joblib.load(mlp_model_path)\n",
    "\n",
    "# Predict on the validation data\n",
    "mlp_predictions = mlp_model.predict(validation_features)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "mlp_accuracy = accuracy_score(validation_labels, mlp_predictions)\n",
    "print(\"MLP - Accuracy:\", mlp_accuracy)\n",
    "\n",
    "# Calculate and print macro F1 score\n",
    "mlp_macro_f1 = f1_score(validation_labels, mlp_predictions, average='macro')\n",
    "print(\"MLP - Macro F1 Score:\", mlp_macro_f1)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"MLP - Classification Report:\\n\", classification_report(validation_labels, mlp_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recombine .pth files for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recombined file saved to: ../src/models/weights/bert_model_recombined.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the split files\n",
    "split_dir = \"../src/models/weights/split_files_bert\"\n",
    "# Output file for recombined weights\n",
    "output_file = \"../src/models/weights/bert_model_recombined.pth\"\n",
    "\n",
    "# Get a sorted list of all split files in the directory\n",
    "split_files = sorted([os.path.join(split_dir, f) for f in os.listdir(split_dir) if f.startswith(\"part_\")])\n",
    "\n",
    "# Recombine the files\n",
    "with open(output_file, \"wb\") as outfile:\n",
    "    for split_file in split_files:\n",
    "        with open(split_file, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "print(f\"Recombined file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recombined file saved to: ../src/models/weights/roberta_model_recombined.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the split files\n",
    "split_dir = \"../src/models/weights/split_files_roberta\"\n",
    "# Output file for recombined weights\n",
    "output_file = \"../src/models/weights/roberta_model_recombined.pth\"\n",
    "\n",
    "# Get a sorted list of all split files in the directory\n",
    "split_files = sorted([os.path.join(split_dir, f) for f in os.listdir(split_dir) if f.startswith(\"part_\")])\n",
    "\n",
    "# Recombine the files\n",
    "with open(output_file, \"wb\") as outfile:\n",
    "    for split_file in split_files:\n",
    "        with open(split_file, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "print(f\"Recombined file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 918/918 [00:00<00:00, 976.82 examples/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8693\n",
      "F1-Score: 0.5235\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import ElectraTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "import torch.nn.functional as F\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='electra')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the Electra Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained Electra model\n",
    "electra_model = load_and_prepare_model('electra', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/electra_model.pth'\n",
    "electra_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "electra_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "electra_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "electra_probs = []\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = electra_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        electra_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro') \n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Record probabilities for ensemble\n",
    "electra_probs_combined = np.concatenate(electra_probs, axis=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 918/918 [00:00<00:00, 998.77 examples/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8344\n",
      "F1-Score: 0.5175\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='bert')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the BERT Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = load_and_prepare_model('bert', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/bert_model_recombined.pth'\n",
    "bert_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "bert_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "bert_probs = []\n",
    "\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        bert_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro') \n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Record probabilities for ensemble\n",
    "bert_probs_combined = np.concatenate(bert_probs, axis=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 918/918 [00:00<00:00, 1869.11 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8627\n",
      "F1-Score: 0.5312\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Add custom module path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from models.transformer_based_models import load_and_prepare_model\n",
    "from data_processing.feature_extraction import prepare_single_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare Dataset\n",
    "# -------------------------------\n",
    "# Tokenize the validation dataset\n",
    "tokenized_dataset = prepare_single_dataset(validation_dataset, model_type='roberta')\n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define batch size\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load the RoBERTa Model\n",
    "# -------------------------------\n",
    "# Define the number of labels (for classification tasks)\n",
    "num_labels = 2\n",
    "\n",
    "# Load the pre-trained RoBERTa model\n",
    "roberta_model = load_and_prepare_model('roberta', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_weights_path = '../src/models/weights/roberta_model_recombined.pth'\n",
    "roberta_model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "# Set model to evaluation mode and move to appropriate device\n",
    "roberta_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "roberta_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Perform Inference\n",
    "# -------------------------------\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "roberta_probs = []\n",
    "\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        # Move batch to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        roberta_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Get predicted labels (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate Model Performance\n",
    "# -------------------------------\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro') \n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "roberta_probs_combined = np.concatenate(roberta_probs, axis=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softvoting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Ensemble Accuracy: 0.8693\n",
      "Soft Voting Ensemble F1-Score: 0.5299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Step 1: Stack probabilities and calculate the mean (soft voting)\n",
    "all_probs = np.stack([logreg_probs, bert_probs_combined, electra_probs_combined, roberta_probs_combined], axis=0)  # Shape: (num_models, num_samples, num_classes)\n",
    "ensemble_probs = np.mean(all_probs, axis=0)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "# Step 2: Predict the final class\n",
    "ensemble_predictions = np.argmax(ensemble_probs, axis=1)  # Shape: (num_samples,)\n",
    "\n",
    "# Ground truth labels (validation_labels)\n",
    "# Assuming validation_labels is a NumPy array of true class labels\n",
    "accuracy = accuracy_score(validation_labels, ensemble_predictions)\n",
    "f1 = f1_score(validation_labels, ensemble_predictions, average='macro')  # Use 'weighted' or 'micro' as needed\n",
    "\n",
    "print(f\"Soft Voting Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Soft Voting Ensemble F1-Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-hallucinations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
